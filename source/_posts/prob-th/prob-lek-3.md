---
title: 「Теорвер」Лекция-3
date: 2024-09-16 10:56:25

tags: [Теория вероятностей]
permalink: prob-lek-3
mathjax: true
published: true
---

### Лекция 3

$X, Y$ - случайные величины, $A,B$ - события

### Условные вер-ти и ожидания
Идея: новая инфа -> обновление вер-тей.

| x                | 1   | 2              | 3              |
|------------------|-----|----------------|----------------|
| $P(X=x)$         | 0.2 | 0.2            | 0.6            |
| $P(X=x\mid X>1)$ | 0   | $\frac{1}{4} $ | $\frac{3}{4} $ |

<!--more-->

#### Определение условной вероятности
> $P(A\mid B) - $ вер-ть события $A$ при условии, что произошло событие $B$.

$$
P(A\mid B) = \frac{P(A\cap B)}{P(B)}
$$

#### Следствия
1. $P(A\cap B) = P(A\mid B)P(B)$
2. Формула полной вер-ти: (pic_1) $$P(A)=P(A\cap B)+P(A\cap \overline{B})$$
$$P(A)=P(A\mid B)P(B)+P(A\mid \overline{B})P(\overline{B})$$
3. $P(A)=\sum\limits_{i=1}^{n}P(A\mid B_i)P(B_i)$

#### Определение индикатора события
> $I$ - индикатор события $B$ - случайная величина, равная $1$, если событие $B$ произошло, и $0$ иначе.

$$I(w)=\begin{cases} 1, & w\in B \\\\ 0, & w\notin B \end{cases}$$

#### Определение условного математического ожидания
> Условное математическое ожидание. Если $P(B)>0$, то $E(X\mid B)=\frac{E(XI_B)}{P(B)}$

#### Свойства
1. $E(X+Y\mid B)=E(X\mid B)+E(Y\mid B)$
2. $E(cX\mid B)=cE(X\mid B)$
3. $E(X)=E(X\mid B)P(B)+E(X\mid \overline{B})P(\overline{B})$
##### Доказательство
$$
E(X)=\frac{E(XI_B)}{P(B)}P(B)+\frac{E(XI_{\overline{B}})}{P(\overline{B})}P(\overline{B})
$$

$$
E(X)=E(XI_B+XI_{\overline{B}})=E(X\cdot 1)=E(X)
$$

Получаем 
$$
E(X)=\sum\limits_{i=1}^{n}E(X\mid B_i)P(B_i)
$$

### Независимость событий
#### Определение независимости событий
> События $A$ и $B$ называются независимыми, если $P(A\cap B)=P(A)P(B)$

#### Теорема
> $A\perp B$ $\Leftrightarrow\begin{cases}
A\perp B \\\\
A\perp \overline{B} \\\\
\overline{A}\perp B \\\\
\overline{A}\perp \overline{B}
\end{cases}$

---

#### Определение
> События набора $A$ называются независимыми в совокупности, если
> - любое событие из набора независимо с любым пересечением остальных событий
> - для $\forall n$ и набора различных индексов $i_1,\ldots,i_n$ верно $P(A_{i_1}\cap\ldots\cap A_{i_n})=P(A_{i_1})\cdot\ldots\cdot P(A_{i_n})$

Эти условия эквивалентны.

### Формула Байеса
$$
P(A\mid B)=\frac{P(A\cap B)}{P(B)}=\frac{P(B\mid A)P(A)}{P(B\mid A)P(A)+P(B\mid \overline{A})P(\overline{A})}
$$

---
### Независимость случайных величин
$X,Y$ - случайные величины.

$\mathcal{A}$ - события, формулируемые в терминах $X$. 
$\\{X>3\\}$, $\\{\cos(X^2)>\sin(5X)\\}$

$\mathcal{B}$ - события, формулируемые в терминах $Y$.

Неформальное определение: $X$ и $Y$ независимы, если $\forall A\in\mathcal{A}$ и $\forall B\in\mathcal{B}$ события $A$ и $B$ независимы.

#### Определение
> Случайные величины $X$ и $Y$ называются независимыми, 
> если $\forall A\subseteq\mathbb{R}$ и $\forall B\subseteq\mathbb{R}$ событие $\\{X\in A\\}$ и $\\{Y\in B\\}$ независимы. 

Для дискретных случайных величин все верно.

####  Теорема
> Случайные величины $X$ и $Y$ независимы если, и только если события $\\{X\leqslant x\\}$ и $\\{Y\leqslant y\\}$ независимы $\forall x,y\in\mathbb{R}$

#### Теорема
> Дискретные случайные величины $X$ и $Y$ независимы, если и только если события $\\{X=x\\}$ и $\\{Y=y\\}$ независимы $\forall x,y\in\mathbb{R}$